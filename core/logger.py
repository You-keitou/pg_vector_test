"""
Logging utilities for data processing operations.
"""

import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional


class DualLogger:
    """æ¨™æº–å‡ºåŠ›ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹ã«ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, log_file: str = "log.txt", level: int = logging.INFO):
        """
        Args:
            log_file: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
        """
        self.log_file = Path(log_file)
        self.logger = logging.getLogger("data_processor")
        self.logger.setLevel(level)

        # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        self.logger.handlers.clear()

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’è¨­å®š
        formatter = logging.Formatter(
            "%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
        )

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        file_handler = logging.FileHandler(self.log_file, encoding="utf-8")
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

        # ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã—ã¦é‡è¤‡ã‚’é˜²ã
        self.logger.propagate = False

    def info(self, message: str):
        """æƒ…å ±ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        self.logger.info(message)

    def warning(self, message: str):
        """è­¦å‘Šãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        self.logger.warning(message)

    def error(self, message: str):
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        self.logger.error(message)

    def debug(self, message: str):
        """ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        self.logger.debug(message)

    def log_processing_start(self, total_rows: int, chunk_strategy: str):
        """å‡¦ç†é–‹å§‹ã®ãƒ­ã‚°"""
        self.info("=" * 60)
        self.info("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        self.info(f"ç·è¡Œæ•°: {total_rows:,}")
        self.info(f"ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥: {chunk_strategy}")
        self.info(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.info("=" * 60)

    def log_processing_progress(
        self, processed: int, total: int, chunks_created: int, elapsed_time: float
    ):
        """å‡¦ç†é€²æ—ã®ãƒ­ã‚°"""
        percentage = (processed / total) * 100
        rate = processed / elapsed_time if elapsed_time > 0 else 0
        eta = (total - processed) / rate if rate > 0 else 0

        self.info(
            f"é€²æ—: {processed:,}/{total:,} ({percentage:.1f}%) | "
            f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆæ•°: {chunks_created:,} | "
            f"å‡¦ç†é€Ÿåº¦: {rate:.1f} rows/sec | "
            f"æ¨å®šæ®‹ã‚Šæ™‚é–“: {eta:.0f}ç§’"
        )

    def log_processing_complete(
        self, processed: int, chunks_created: int, elapsed_time: float
    ):
        """å‡¦ç†å®Œäº†ã®ãƒ­ã‚°"""
        self.info("=" * 60)
        self.info("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        self.info(f"å‡¦ç†è¡Œæ•°: {processed:,}")
        self.info(f"ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {chunks_created:,}")
        self.info(f"ç·å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        self.info(f"å¹³å‡å‡¦ç†é€Ÿåº¦: {processed / elapsed_time:.1f} rows/sec")
        self.info(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.info("=" * 60)

    def log_error(self, error: Exception, context: Optional[str] = None):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"""
        if context:
            self.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({context}): {str(error)}")
        else:
            self.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(error)}")

    def log_statistics(self, stats: dict):
        """çµ±è¨ˆæƒ…å ±ã®ãƒ­ã‚°"""
        self.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ:")
        for key, value in stats.items():
            self.info(f"  - {key}: {value:,}")

    def log_embedding_info(self, embedding_info: dict):
        """embeddingæƒ…å ±ã®ãƒ­ã‚°"""
        if embedding_info["has_embeddings"]:
            self.info("âœ… Embeddings generated successfully!")
            self.info(
                f"  - Embedding dimension: {embedding_info['embedding_dimension']}"
            )
            self.info(f"  - Sample text: {embedding_info['sample_text']}")
        else:
            self.warning("âš ï¸  No embeddings found")

    def log_commit(self, processed_rows: int, total_chunks: int):
        """ã‚³ãƒŸãƒƒãƒˆæ™‚ã®ãƒ­ã‚°"""
        self.info(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚³ãƒŸãƒƒãƒˆå®Œäº†: {processed_rows} è¡Œå‡¦ç†æ¸ˆã¿, {total_chunks} ãƒãƒ£ãƒ³ã‚¯ä½œæˆæ¸ˆã¿")

    def log_batch_split(self, total_texts: int, batch_size: int):
        """ãƒãƒƒãƒåˆ†å‰²æ™‚ã®ãƒ­ã‚°"""
        num_batches = (total_texts + batch_size - 1) // batch_size
        self.info(f"ğŸ“¦ å¤§ããªãƒãƒƒãƒã‚’åˆ†å‰²: {total_texts} ãƒ†ã‚­ã‚¹ãƒˆ â†’ {num_batches} ãƒãƒƒãƒ (ã‚µã‚¤ã‚º: {batch_size})")

    def log_rate_limit_retry(self, attempt: int, wait_time: float):
        """Rate limitå†è©¦è¡Œæ™‚ã®ãƒ­ã‚°"""
        self.warning(f"â³ Rate limitæ¤œå‡º - {wait_time:.1f}ç§’å¾Œã«å†è©¦è¡Œ (è©¦è¡Œå›æ•°: {attempt}/5)")
